# Agricultural Data Engineering Pipeline ğŸŒ¾

A modern data pipeline leveraging AWS S3, Databricks, and MongoDB Atlas to process, analyze, and store agricultural metrics across countries.

## Architecture Overview ğŸ“Š
- Data Storage: AWS S3
- Processing Engine: Databricks (PySpark)
- Analytics Database: MongoDB Atlas
- Visualization: Matplotlib/Seaborn

## Infrastructure Setup âš™ï¸

### Cloud Components
- Databricks Runtime 15.4 LTS ğŸ”§
- AWS S3 Bucket â˜ï¸
- MongoDB Atlas Cluster ğŸ—„ï¸

### Dependencies ğŸ“š
```python
pyspark
pymongo
matplotlib 
seaborn
pandas
numpy
certifi
```

### Authentication Configuration ğŸ”‘
- IAM Role for S3 Access
- Reference: [Databricks S3 Access Guide](https://gist.github.com/ifyjakande/3db382f383078832abd81f5cf3449ad6)

## Data Pipeline ğŸ”„
### Sources 
- `Cereal_Yield.xls`: Agricultural yield metrics ğŸŒ½
- `Land_Use.xls`: Geographical data ğŸ—ºï¸

### Processing Features
- ETL Workflow & Data Cleaning ğŸ§¹
- Statistical Analysis ğŸ“Š
- Automated Insights Generation ğŸ¤–
- Time Series Visualization ğŸ“ˆ
- MongoDB Integration for Results Storage ğŸ’¾

## System Architecture ğŸ—ï¸
![Pipeline Architecture](/reco-system/architecture.jpg)

## Sample Output ğŸ“Š
![Pipeline Results](/reco-system/mongodb-data1.jpg)

## License ğŸ“„
MIT
